#+title: Machine Learning with Python
#+author: Nick Martin
#+email: nmartin84@gmail.com
#+created: [2021-01-23 13:54]
#+source: https://www.tutorialspoint.com/machine_learning_with_python/index.htm

* Machine Learning with Python

** Basics

Machine Learning basically allows the computer to make sense of the data. It
extracts patterns out of data using a set of algorithm and or method.

The idea behind Machine Learning, is to help humans make decisions, based on
data.

*** Machine Learning Model :ATTACH:
:PROPERTIES:
:ID:       741e7e44-ec35-4155-bbd9-80483d28816d
:END:

Before discussing the machine learning model, we must need to understand the
following formal definition of ML given by professor Mitchell:

#+begin_quote
“A computer program is said to learn from experience (E) with respect to some
class of tasks (T) and performance measure (P), if its performance at tasks in
(T), as measured by (P), improves with experience (E).”
#+end_quote

The above quote basically calls-out the three elements to the machine learning
objective, knowing this we can define Machine Learning as:
+ Improve their performance (P)
+ At executing some task (T)
+ Over time with Experience (E)

[[file:../.attach/74/1e7e44-ec35-4155-bbd9-80483d28816d/machine_learning_model.jpg]]

**** Task (T)

We can define (T "[[[[file:../fleeting/202101231438-task.org][Task]]]]") as the real-world problem to be solved. The problem
which we are trying to solve can be finding housing prices, to marketing
strategy, etc. On the other hand, if we talk about Machine Learning, the
definition for (T) would be different, because our Task that we are trying to
solve is different than the normal.

So basically a (T) Task is said to be based on Machine Learning, when the (T) is
based on the process, and the system must follow for operating on data points.
The examples for Machine Learning are Classification, Regression, Structured
annotation, Clustering, Transcription, etc.

**** Experience (E)

So (E) is essentially our data that's used to build knowledge, which in turn is
applied towards solving our (T) by analyzing and finding patterns.

**** Performance (P)

Our (P) is a measure of how well our Machine Learning algorithm is performing
based on our expectations given its output. (P) is basically a quantitative
metric that tells how a model is performing Task (T), using its Experience (E).
There are many metrics that help understand the Machine Learning performance
(P), such as; accuracy score, F1 score, confusion matrix, precision, recall,
sensitivity, etc...

*** Challenges in Machine Learning

Essentially Machine Learning is still in it's early primes, and has many
improvements to be made, such as in the following areas:
+ quality of data :: Having good-quality data for ML algorithms is one of the
  biggest challenges. Use of low-quality data leads to the problems related to
  data preprocessing and feature extraction.
+ Time-Consuming task :: Another challenge faced by ML models is the consumption
  of time especially for data acquisition, feature extraction and retrieval.
+ Lack of specialist persons :: As ML technology is still in its infancy stage,
  availability of expert resources is a tough job.
+ No clear objective for formulating business problems :: Having no clear
  objective and well-defined goal for business problems is another key
  challenge for ML because this technology is not that mature yet.
+ Issue of overfitting & underfitting :: If the model is overfitting or
  underfitting, it cannot be represented well for the problem.
+ Curse of dimensionality :: Another challenge ML model faces is too many
  features of data points. This can be a real hindrance.
+ Difficulty in deployment :: Complexity of the ML model makes it quite
  difficult to be deployed in real life.

*** Applications of Machine Learning

We are in the midst golden-age of AI and Machine Learning. Some example
applications for machine learning are:
+ Emotion analysis
+ Sentiment analysis
+ Error detection and prevention
+ Weather forecasting and prediction
+ Stock market analysis and forecasting
+ Speech synthesis
+ Speech recognition
+ Customer segmentation
+ Object recognition
+ Fraud detection
+ Fraud prevention
+ Recommendation of products to customer in online shopping

** Methods for Machine Learning
:LOGBOOK:
CLOCK: [2021-01-23 Sat 19:47]--[2021-01-23 Sat 20:12] =>  0:25
:END:

Basically there are multiple Machine Learning algorithms, techniques and methods
that can be applied to build models for solving real-life problems by using data.

*** Different types of Methods

The following are various ML Methods based on some broad categories:

**** Supervised Learning

[[[[file:../fleeting/202101232318-supervised_learning.org][supervised learning]]]]: this is basically taking the input data, and
associating it with the expected outcome results. Some examples of this:
- x: input variables
- y: output variable
- ~Y=f(x)~

The idea would be to approximate the output given the input data we are given.
Supervised algorithms can be categorized as:
+ Classification
+ Regression

***** Classification

Predict the categorical output labels or response for the given input data. The
output will be based on what the model has learned in the training phase.

***** Regression

Predict output labels or responses that are continues numeric values, for the
given input data. The output will be based on what the model has learned in its
training phase.

**** Unsupervised Learning

Opposite of supervised Machine Learning algorithms, which basically means we
have no supervisor to provide any sort of guidance. This is for instance, which
we do not have the liberty of having pre-labeled training data and we want to
extract useful patterns from input data.

For instance, say you have input variables, but there would be no output
variable and the algorithms need to discover the interesting pattern in data for
learning.

Examples of [[[[file:../fleeting/202101240943-unsupervised_learning.org][unsupervised learning]]]] are: k-means clustering, k-nearest neighbors,
etc...

Unsupervised learning is categorized in the following; Clustering, Association,
Dimensionality Reduction and Anomaly Detection.

***** Clustering

One of the most powerful unsupervised Machine Learning methods. Used to find
similarity as well as relationship patterns among data samples and then cluster
those samples into groups based on features. An example of this would be
grouping customers by their purchasing behavior.

***** Association

Essentially is another useful algorithm for finding patterns which further
represents the interesting relationships between various items. An example of
this, is to analyze a customers shopping patterns.

***** Dimensionality Reduction

Reduce number of feature variables for each data sample by selecting set of
principal or representative features. Question arises of why we need to reduce
the [[[[file:../fleeting/202101232008-dimensionality.org][dimensionality]]]]? The reason behins is the problem of [[[[file:../fleeting/202101232009-feature_space_complexity.org][feature space
complexity]]]] which arises when we start analyzing and extracting millions of
features from data samples. This problem generally refers to "curse of
[[[[file:../fleeting/202101232008-dimensionality.org][dimensionality]]]]". Principal Component Analysis, K-nearest neighbors and
discriminant analysis are some of the popular algorithms for this purpose.

***** Anomaly Detection

Used to find the occurrences of rare events or observations that generally do
not occur. Some of the unsupervised algorithms like clustering, KNN can detect
anomalies based on the data and its features.

**** Semi-Supervised Learning

Neither supervised nor unsupervised, but a mixture of both. They tend to use a
small amount of pre-labeled annotated data and large unsupervised learning
component, i.e. lots of unlabeled data for training.

+ The first and simple approach is to build with an initial small amount of
  labeled and annotated data, and then build the unsupervised model by applying
  the same to the large amounts of unlabeled data to get more labeled samples.
  Now, train the model on them and repeat the process.
+ Second approach needs some extra efforts. We first use unsupervised methods to
  cluster similar data samples, annotate these groups and then use a combination
  of this information to train the model.

**** Reinforcement Learning

These methods are rarely used. In these type of algorithms, there would be an
agent that we want to train over a period of time so that it can interact with a
specific environment. The agent will follow a set of strategies for interacting
with the environment, and then after observing it will take actions regards the
current state of the environment. Following are the main steps for reinforcement
learning:
- Step 1 :: First, we need to prepare an agent with some initial set of
 strategies.
- Step 2 :: Then observe the environment and its current state.
- Step 3 :: Next, select the optimal policy regards the current state of the
 environment and perform important action.
- Step 4 :: Now, the agent can get corresponding reward or penalty as per
 accordance with the action taken by it in previous step.
- Step 5 :: Now, we can update the strategies if it is required so.
- Step 6 :: At last, repeat steps 2-5 until the agent got to learn and adopt the
 optimal policies.

*** Tasks suited for Machine Learning :ATTACH:
:PROPERTIES:
:ID:       626daac0-c76e-47ff-8a83-a939263a1224
:END:

[[file:../.attach/62/6daac0-c76e-47ff-8a83-a939263a1224/task_for_ml_problems.jpg]]

**** Batch Learning

In many cases, we have end-to-end Machine Learning systems in which we need to
train the model in one go by using whole available training data. Such kind of
learning method or algorithm is called Batch or Offline learning. It is called
Batch or Offline learning because it is a one-time procedure and the model will
be trained with data in one single batch. The following are the main steps of
Batch learning methods:
- Step 1 :: First, we need to collect all the training data for start training
  the model.
- Step 2 :: Now, start the training of model by providing whole training data in
  one go.
- Step 3 :: Next, stop learning/training process once you got satisfactory
  results/performance.
- Step 4 :: Finally, deploy this trained model into production. Here, it will
  predict the output for new data sample.

**** Online Learning

Is the opposite of batch learning, and the data is provided in multiple
incremental batches, called mini-batches, to the algorithm. Following are the
main steps for online learning:
- Step 1 :: First, we need to collect all the training data for starting
  training of the model.
- Step 2 :: Now, start the training of model by providing a mini-batch of
  training data to the algorithm.
- Step 3 :: Next, we need to provide the mini-batches of training data in
  multiple increments to the algorithm.
- Step 4 :: As it will not stop like batch learning hence after providing whole
  training data in mini-batches, provide new data samples also to it.
- Step 5 :: Finally, it will keep learning over a period of time based on the
  new data samples.

**** Based on Generalization Approach

***** Instance Based Learning
:LOGBOOK:
CLOCK: [2021-01-23 Sat 21:33]--[2021-01-23 Sat 22:10] =>  0:37
:END:

Instance based is one of the useful Machine Learning methods, by doing
generalization based on the input data. It is opposite of the previously
mentioned learning methods, in that this kind of learning involves Machine
Learning systems as well as methods that uses the raw data points themselves to
draw the outcomes for newer data samples without building an explicit model on
training data.

Or in simpler terms, it'll basically start working by looking at the input data
and then use a similarity metric, it will generalize and predict the new data
points.

***** Model Based Learning

An iterative process takes place on the Machine Learning models that are built
based on various model parameters, called [[[[file:../fleeting/202101232139-hyperparameters.org][hyperparameters]]]] and in which input
data is used to extract features.

In this model, the [[[[file:../fleeting/202101232139-hyperparameters.org][hyperparameters]]]] are used to optimize based on various
model validation techniques.

** Data loading for Machine Learning Projects

The first and foremost thing to start any Machine Learning project, is *data*. The
most common format for any data to be used is *csv*.

*** Considerations while loading CSV data

When loading data from CSV into our Machine Learning project, it is imperative
that we take into consideration the following key items:

**** File Header

The header contains the metadata for the given columns, to help categorize or to
put in summarization what the data for the given column entails. One should keep
in mind when dealing with headers:
+ If a header exists, the header data will be taken into account and labeled
  accordingly.
+ If no header exists, then we need to define our header names to categorize our
  data.

**** Comments

We need to consider that comments may be added to the csv file, comments are
added at the beginning of a line and notated with a ~#~ hashtag.

**** Delimeter

Obviously, the delimeter for which will tell the interpreter how to separate the
data from one another. This is commonly assigned to the ~,~ character.

**** Quotes

It's essential to keep in mind of quotes, because quotes are used to group
together a string of words separated by white-spaces, which when not quoted could
lead to issues.

*** Methods to load CSV data file

So basically this will cover a few functions which can be applied to loading CSV
data into python.

**** Loading CSV with the Python Standard Library

The first and most used approach is using the [[[[file:../programming/python/202101021658-csv.org][csv]]]] module provided with
Python and the ~reader()~ function.

#+begin_example
import csv
import numpy as np

path=r"c:\iris.csv"

with open(path, 'r') as f:
    reader = csv.reader(f, delimter=',')
    headers = next(reader)
    data = list(reader)
    data = np.array(data).astype(float)
#+end_example

**** Load CSV with NumPy

#+begin_example
from numpy import loadtxt
path = r"C:\pima-indians-diabetes.csv"
datapath= open(path, 'r')
data = loadtxt(datapath, delimiter=",")
print(data.shape)
print(data[:3])
#+end_example

**** Load CSV with Pandas

#+begin_example
from pandas import read_csv
path = r"C:\iris.csv"
data = read_csv(path)
print(data.shape)
print(data[:3])
#+end_example

** Understanding Data with Statistics
:LOGBOOK:
CLOCK: [2021-01-23 Sat 22:10]
:END:

While working with Machine Learning, two aspect that are often forgotten are the
importance of mathematics and data. Before we can really use Machine Learning to
address our problem, we need to have an understanding of our data first. So
there are two ways to help us understand our data; [[[[file:../statistics/202101212128-statistics.org][statistics]]]] and
*visualization*.

This section is going to cover how we can understand our data through
*statistics*.

*** Looking at Raw Data

The first step is looking at your raw data to get a better understanding of it.
For instance, looking at "India's Diabetes Dataset" and look at the first 50
rows to get a better understanding of our data:

#+begin_example
from pandas import read_csv
path = r"C:\pima-indians-diabetes.csv"
headernames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=headernames)
print(data.head(50))

preg   plas  pres    skin  test  mass   pedi    age      class
0      6      148     72     35   0     33.6    0.627    50    1
1      1       85     66     29   0     26.6    0.351    31    0
2      8      183     64      0   0     23.3    0.672    32    1
3      1       89     66     23  94     28.1    0.167    21    0
4      0      137     40     35  168    43.1    2.288    33    1
5      5      116     74      0   0     25.6    0.201    30    0
6      3       78     50     32   88    31.0    0.248    26    1
7     10      115      0      0   0     35.3    0.134    29    0
8      2      197     70     45  543    30.5    0.158    53    1
9      8      125     96      0   0     0.0     0.232    54    1
10     4      110     92      0   0     37.6    0.191    30    0
11    10      168     74      0   0     38.0    0.537    34    1
12    10      139     80      0   0     27.1    1.441    57    0
13     1      189     60     23  846    30.1    0.398    59    1
14     5      166     72     19  175    25.8    0.587    51    1
15     7      100      0      0   0     30.0    0.484    32    1
16     0      118     84     47  230    45.8    0.551    31    1
17     7      107     74      0   0     29.6    0.254    31    1
18     1      103     30     38  83     43.3    0.183    33    0
19     1      115     70     30  96     34.6    0.529    32    1
20     3      126     88     41  235    39.3    0.704    27    0
21     8       99     84      0   0     35.4    0.388    50    0
22     7      196     90      0   0     39.8    0.451    41    1
23     9      119     80     35   0     29.0    0.263    29    1
24    11      143     94     33  146    36.6    0.254    51    1
25    10      125     70     26  115    31.1    0.205    41    1
26     7      147     76      0   0     39.4    0.257    43    1
27     1       97     66     15  140    23.2    0.487    22    0
28    13      145     82     19  110    22.2    0.245    57    0
29     5      117     92      0   0     34.1    0.337    38    0
30     5      109     75     26   0     36.0    0.546    60    0
31     3      158     76     36  245    31.6    0.851    28    1
32     3       88     58     11   54    24.8    0.267    22    0
33     6       92     92      0   0     19.9    0.188    28    0
34    10      122     78     31   0     27.6    0.512    45    0
35     4      103     60     33  192    24.0    0.966    33    0
36    11      138     76      0   0     33.2    0.420    35    0
37     9      102     76     37   0     32.9    0.665    46    1
38     2       90     68     42   0     38.2    0.503    27    1
39     4      111     72     47  207    37.1    1.390    56    1
40     3      180     64     25   70    34.0    0.271    26    0
41     7      133     84      0   0     40.2    0.696    37    0
42     7      106     92     18   0     22.7    0.235    48    0
43     9      171    110     24  240    45.4    0.721    54    1
44     7      159     64      0   0     27.4    0.294    40    0
45     0      180     66     39   0     42.0    1.893    25    1
46     1      146     56      0   0     29.7    0.564    29    0
47     2       71     70     27   0     28.0    0.586    22    0
48     7      103     66     32   0     39.1    0.344    31    1
49     7      105      0      0   0     0.0     0.305    24    0
#+end_example

So we can observe from the above output, that the first row gives us the index
number which we can use to reference back to a specific observation.

We should also keep in mind the size of our datasets, to see if; The dataset is
very large and could take our algorithm a long term to train, or the dataset is
very small and would provide poor results.

Here with this example from python we can display, or get a rough idea of how
big our dataset is:

#+begin_example
from pandas import read_csv
path = r"C:\iris.csv"
data = read_csv(path)
print(data.shape)

(150, 4)
#+end_example

*** Getting Each Attribute's Data Type

Another key important thing is to know what type of data you are dealing with in
each column, this is particularly important to know so your algorithm is
applying the correct functions according to the respective datatype. Here's an
example of how to print out your datatype for your columns.

#+begin_example
from pandas import read_csv
path = r"C:\iris.csv"
data = read_csv(path)
print(data.dtypes)

sepal_length  float64
sepal_width   float64
petal_length  float64
petal_width   float64
dtype: object
#+end_example

*** Statistical Summary of Data

Many times we would need to review the summaries of our data, which we can do so
with the help of the ~describe()~ function in Pandas that provides the following 8
statistical properties of each & every date attribute:
1. Count
2. Mean
3. Standard Deviation
4. Minimum Value
5. Maximum Value
6. 25%
7. Median i.e. 50%
8. 75%

#+begin_example
from pandas import read_csv
from pandas import set_option
path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=names)
set_option('display.width', 100)
set_option('precision', 2)
print(data.shape)
print(data.describe())

(768, 9)
         preg      plas       pres      skin      test        mass       pedi      age      class
count 768.00      768.00    768.00     768.00    768.00     768.00     768.00    768.00    768.00
mean    3.85      120.89     69.11      20.54     79.80      31.99       0.47     33.24      0.35
std     3.37       31.97     19.36      15.95    115.24       7.88       0.33     11.76      0.48
min     0.00        0.00      0.00       0.00      0.00       0.00       0.08     21.00      0.00
25%     1.00       99.00     62.00       0.00      0.00      27.30       0.24     24.00      0.00
50%     3.00      117.00     72.00      23.00     30.50      32.00       0.37     29.00      0.00
75%     6.00      140.25     80.00      32.00    127.25      36.60       0.63     41.00      1.00
max    17.00      199.00    122.00      99.00    846.00      67.10       2.42     81.00      1.00
#+end_example

*** Reviewing Class Distribution

[[[[file:../fleeting/202101232301-class_distribution.org][class distribution]]]] statistics is useful in classification problems where we
need to know the balance of class values. It's important to know class value
distribution because if we have high imbalanced class distribution, i.e. one
class is having lots more observations than another class, then it ma need
special handling of data preperation stage of our Machine Learning project.

#+begin_example
from pandas import read_csv
path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=names)
count_class = data.groupby('class').size()
print(count_class)

Class
0  500
1  268
dtype: int64
#+end_example

We can see from the example output above, that our class 0 observations are
nearly double that of class 1.

*** Reviewing Correlation between Attributes

The relationship between two variables is called correlation, or better yet
[[[[file:../fleeting/202101232304-variable_correlation.org][variable correlation]]]]. In [[[[file:../statistics/202101212128-statistics.org][statistics]]]], the most efficient method for
calculation correlation is [[[[file:../fleeting/202101232306-pearsons_correlation_coefficient.org][pearsons correlation coefficient]]]]. It can have
three values as follows:

+ Coefficient value = 1 :: Represents full positive correlation between variables.
+ Coefficient value = -1 :: Represents full negative correlation between variables.
+ Coefficient value = 0 :: It represents no correlation at all between variables.

It's wise to also look at you coefficient variables, since some Machine Learning
algorithms, such as linear regression and logistic regression perform poorly if
we have highly correlated attributes. In python we can easily calculate a
correlation matrix of dataset attributes with theh elp of the ~corr()~ function on
Pandas DataFrame.

#+begin_example
from pandas import read_csv
from pandas import set_option
path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=names)
set_option('display.width', 100)
set_option('precision', 2)
correlations = data.corr(method='pearson')
print(correlations)

preg     plas     pres     skin     test      mass     pedi       age      class
preg     1.00     0.13     0.14     -0.08     -0.07   0.02     -0.03       0.54   0.22
plas     0.13     1.00     0.15     0.06       0.33   0.22      0.14       0.26   0.47
pres     0.14     0.15     1.00     0.21       0.09   0.28      0.04       0.24   0.07
skin    -0.08     0.06     0.21     1.00       0.44   0.39      0.18      -0.11   0.07
test    -0.07     0.33     0.09     0.44       1.00   0.20      0.19      -0.04   0.13
mass     0.02     0.22     0.28     0.39       0.20   1.00      0.14       0.04   0.29
pedi    -0.03     0.14     0.04     0.18       0.19   0.14      1.00       0.03   0.17
age      0.54     0.26     0.24     -0.11     -0.04   0.04      0.03       1.00   0.24
class    0.22     0.47     0.07     0.07       0.13   0.29      0.17       0.24   1.00
#+end_example

The example above gives an example of correlation between all the pairs of the
attribute in the dataset.

*** Reviewing Skew of Attribute Distribution

So essentially it's defined as a distribution that is assumed to be Gaussian,
but appears to be distorted or shifted in one direction or another. Reviewing
for skewness is important for one of the following reasons:
+ Presence of skewness needs to be addressed with the data before its injected
  into the Machine Learning project to get more accuracy from our model.
+ Most of Machine Learning algorithms assumes that data has a Gauassian
  distribution (ie: either normal of bell curved data).

With python and Pandas, we can use the ~skew()~ function to calculate the skewness
of our data.

#+begin_example

from pandas import read_csv
path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=names)
print(data.skew())

preg   0.90
plas   0.17
pres  -1.84
skin   0.11
test   2.27
mass  -0.43
pedi   1.92
age    1.13
class  0.64
dtype: float64
#+end_example

** Understanding data with Visualization :ATTACH:
:PROPERTIES:
:ID:       f0229bad-2e4e-4157-8d4c-2883e4ca6858
:END:

Visualization is a good aid to see how our data correlates with one another, and
is a good helping aide to understanding our data as well.

*** Histograms
[[[[file:../fleeting/202101241142-histograms.org][histograms]]]] group the data into bins, and is the fastest way to get an idea
about the distribution of each attribute in the dataset. Here are some
characteristics for a histogram:
+ Provides a count for number of observations for each bin.
+ From the shape we can usually see if the data is Gaussian, Skewed or Exponential.
+ Also helps us to see possible outliers.

The below is an example of a histogram using the Indian Diabetes Dataset:
#+begin_example
from matplotlib import pyplot
from pandas import read_csv
path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=names)
data.hist()
pyplot.show()
#+end_example

[[file:../.attach/f0/229bad-2e4e-4157-8d4c-2883e4ca6858/graph.jpg]]

From this, we can observe that perhaps age, pedi and test attribute may have
[[[[file:../fleeting/202101241201-exponential_distribution.org][exponential distribution]]]] while mass and plas have [[[[file:../fleeting/202101241200-gaussian_distribution.org][gaussian distribution]]]].

*** Density Plots :ATTACH:

[[[[file:../fleeting/202101241201-density_plots.org][density plots]]]] is another quick and easy technique for getting each
attributes distribution. It's very similar to a histogram, except that it draws
a smooth curve between bins.

#+begin_example
from matplotlib import pyplot
from pandas import read_csv
path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=names)
data.plot(kind='density', subplots=True, layout=(3,3), sharex=False)
pyplot.show()
#+end_example

[[file:../.attach/f0/229bad-2e4e-4157-8d4c-2883e4ca6858/density.jpg]]

From the above output, the difference between Density plots and Histograms can
be easily understood.

*** Box and Whisker Plots :ATTACH:

[[[[file:../fleeting/202101241204-boxplots.org][boxplots]]]], is another useful visualization technique for each attributes
distribution. Some of the characteristics for boxplots are:
- It is univariate in nature and summarizes the distribution of each attribute.
- It draws a line for the middle value i.e. for median.
- It draws a box around the 25% and 75%.
- It also draws whiskers which will give us an idea about the spread of the
  data.
- The dots outside the whiskers signifies the outlier values. Outlier values
  would be 1.5 times greater than the size of the spread of the middle data.

#+begin_example
from matplotlib import pyplot
from pandas import read_csv
path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=names)
data.plot(kind='box', subplots=True, layout=(3,3), sharex=False,sharey=False)
pyplot.show()
#+end_example

[[file:../.attach/f0/229bad-2e4e-4157-8d4c-2883e4ca6858/mass.jpg]]

From the example above, we can see that age, skin and test appear to be skewed
towards smaller values.
*** Multivariate Plots: Interaction Among Multiple Variables

Multivariate is another type of visualization aide, which applies multiple
attributes in a single plot so we can get an understand of the relation between
our attributes.

**** Correlation Matrix Plot :ATTACH:

[[[[file:../fleeting/202101232304-variable_correlation.org][variable correlation]]]] in an indication about the changes between two
variables. We can plot a correlation matrix plot to see which variable is having
a high or low correlation in respect to another variable.

#+begin_example
from matplotlib import pyplot
from pandas import read_csv
import numpy
Path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(Path, names=names)
correlations = data.corr()
fig = pyplot.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(correlations, vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = numpy.arange(0,9,1)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_xticklabels(names)
ax.set_yticklabels(names)
pyplot.show()
#+end_example

[[file:../.attach/f0/229bad-2e4e-4157-8d4c-2883e4ca6858/class.jpg]]

From the above output of correlation matrix, we can see that it is symmetrical
i.e. the bottom left is same as the top right. It is also observed that each
variable is positively correlated with each other.

**** Scatter Matrix Plot :ATTACH:

[[[[file:../fleeting/202101241224-scatter_plots.org][scatter plots]]]] shows how much one variable is affected by another or the
relationship between them with the help of dots in two dimensions. Scatter plots
are very much like line graphs in the concept that they use horizontal and
vertical axes to plot data points.

#+begin_example
from matplotlib import pyplot
from pandas import read_csv
from pandas.tools.plotting import scatter_matrix
path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=names)
scatter_matrix(data)
pyplot.show()
#+end_example

[[file:../.attach/f0/229bad-2e4e-4157-8d4c-2883e4ca6858/plot_scatter_matrix.jpg]]

** Preparing data

First and foremost, we must have clean data, that's comprehensible, and
applicable to applying towards our solution, or to finding the solution of our
problem.

*** Data Pre-Processing

Pre-processing is the method of which we clean-up and prepare our data to be
inputted into our Machine Learning algorithm. This process is crucial to ensure
that we have the appropriate data in a cleaned up format.

There are a few methods of which we can follow through on this process, of those
we have:

**** Scaling

Most probable, our dataset presents attributes in various scales, which is not
well formed for our algorithm. So the first step in this process is rescaling.

In this example we will rescale the data of Pima Indians Diabetes dataset which
we used earlier. First, the CSV data will be loaded (as done in the previous
chapters) and then with the help of MinMaxScaler class, it will be rescaled in
the range of 0 and 1.

#+begin_example
from pandas import read_csv
from numpy import set_printoptions
from sklearn import preprocessing
path = r'C:\pima-indians-diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv(path, names=names)
array = dataframe.values
#+end_example

Now we use ~MinMaxScaler~ class to rescale the data in the range of 0 and 1.

#+begin_example
data_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))
data_rescaled = data_scaler.fit_transform(array)
#+end_example

We can also summarize the data for output as per our choice. Here, we are
setting the precision to 1 and showing the first 10 rows in the output.

#+begin_example
set_printoptions(precision=1)
print ("\nScaled data:\n", data_rescaled[0:10])
#+end_example

#+begin_example
Scaled data:
[
   [0.4 0.7 0.6 0.4 0.  0.5 0.2 0.5 1. ]
   [0.1 0.4 0.5 0.3 0.  0.4 0.1 0.2 0. ]
   [0.5 0.9 0.5 0.  0.  0.3 0.3 0.2 1. ]
   [0.1 0.4 0.5 0.2 0.1 0.4 0.  0.  0. ]
   [0.  0.7 0.3 0.4 0.2 0.6 0.9 0.2 1. ]
   [0.3 0.6 0.6 0.  0.  0.4 0.1 0.2 0. ]
   [0.2 0.4 0.4 0.3 0.1 0.5 0.1 0.1 1. ]
   [0.6 0.6 0.  0.  0.  0.5 0.  0.1 0. ]
   [0.1 1.  0.6 0.5 0.6 0.5 0.  0.5 1. ]
   [0.5 0.6 0.8 0.  0.  0.  0.1 0.6 1. ]
]
#+end_example

From the above output, all the data got rescaled into the range of 0 and 1.

**** Normalization

Another method is what we call normalization, where we take our dataset and
rescale each row to be a value of max 1. It is mainly useful in Spare dataset
where we have lots of zeros. We can rescale with the help of the scikit-learn
python library.

Some types of normalization include:

***** L1 Normalization

It may be defined as the normalization technique that modifies the dataset
values in a way that in each row the sum of the absolute values will always be
up to 1. It is also called Least Absolute Deviations.

#+begin_example
from pandas import read_csv
from numpy import set_printoptions
from sklearn.preprocessing import Normalizer
path = r'C:\pima-indians-diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv (path, names=names)
array = dataframe.values
#+end_example

Now, we can use the ~Normalize~ class with L1 to normalize the data.

#+begin_example
Data_normalizer = Normalizer(norm='l1').fit(array)
Data_normalized = Data_normalizer.transform(array)
#+end_example

We can also summarize the data for output as per our choice. Here, we are
setting the precision to 2 and showing the first 3 rows in the output.

#+begin_example
set_printoptions(precision=2)
print ("\nNormalized data:\n", Data_normalized [0:3])
#+end_example

#+begin_example
Normalized data:
[
   [0.02 0.43 0.21 0.1  0. 0.1  0. 0.14 0. ]
   [0.   0.36 0.28 0.12 0. 0.11 0. 0.13 0. ]
   [0.03 0.59 0.21 0.   0. 0.07 0. 0.1  0. ]
]
#+end_example

***** L2 Normalization

It may be defined as the normalization technique that modifies the dataset
values in a way that in each row the sum of the squares will always be up to 1.
It is also called least squares.

#+begin_example
from pandas import read_csv
from numpy import set_printoptions
from sklearn.preprocessing import Normalizer
path = r'C:\pima-indians-diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv (path, names=names)
array = dataframe.values
#+end_example

New, we use the L2 ~Normalizer~ class to normalize the data.

#+begin_example
Data_normalizer = Normalizer(norm='l2').fit(array)
Data_normalized = Data_normalizer.transform(array)
#+end_example

We can also summarize the data for output as per our choice. Here, we are
setting the precision to 2 and showing the first 3 rows in the output.

#+begin_example
set_printoptions(precision=2)
print ("\nNormalized data:\n", Data_normalized [0:3])
#+end_example

#+begin_example
Normalized data:
[
   [0.03 0.83 0.4  0.2  0. 0.19 0. 0.28 0.01]
   [0.01 0.72 0.56 0.24 0. 0.22 0. 0.26 0.  ]
   [0.04 0.92 0.32 0.   0. 0.12 0. 0.16 0.01]
]
#+end_example

***** Binarization

Essentially this will allow us to transform our data into binary. We set a value
to our binary threshold, and then anything above or below our thresholds will
essentially be converted to ~1.0~ or ~0.0~. This technique is useful for when we
have probabilities in our datasets and want to convert them into crisp values.

#+begin_example
from pandas import read_csv
from sklearn.preprocessing import Binarizer
path = r'C:\pima-indians-diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv(path, names=names)
array = dataframe.values
#+end_example

Now we can use the ~Binarize~ class to convert our data.

#+begin_example
binarizer = Binarizer(threshold=0.5).fit(array)
Data_binarized = binarizer.transform(array)
#+end_example

Here, we will show the first 5 rows in our dataset.

#+begin_example
print ("\nBinary data:\n", Data_binarized [0:5])
#+end_example

#+begin_example
Binary data:
[
   [1. 1. 1. 1. 0. 1. 1. 1. 1.]
   [1. 1. 1. 1. 0. 1. 0. 1. 0.]
   [1. 1. 1. 0. 0. 1. 1. 1. 1.]
   [1. 1. 1. 1. 1. 1. 0. 1. 0.]
   [0. 1. 1. 1. 1. 1. 1. 1. 1.]
]
#+end_example

***** Standarization

Standardization is useful for converting data with a Gaussian distribution. It
differs the mean and Standard Deviation to a standard Gaussian distribution
with a mean of 0 and a SD of 1. This is useful in linear regression, logistic
regression that assumes a Gaussian distribution in input dataset and produce
better results with rescales data. We can standarize the data with the help of
the ~StandardScaler~ class from the scikit-learn Python Library.

#+begin_example
from sklearn.preprocessing import StandardScaler
from pandas import read_csv
from numpy import set_printoptions
path = r'C:\pima-indians-diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv(path, names=names)
array = dataframe.values
#+end_example

Now, we use the ~StanardScaler~ class to rescale the data.

#+begin_example
data_scaler = StandardScaler().fit(array)
data_rescaled = data_scaler.transform(array)
#+end_example

Here we are setting the precision to 2, and displaying the first 5 rows.

#+begin_example
set_printoptions(precision=2)
print ("\nRescaled data:\n", data_rescaled [0:5])
#+end_example

#+begin_example
Rescaled data:
[
   [ 0.64  0.85  0.15  0.91 -0.69  0.2   0.47  1.43  1.37]
   [-0.84 -1.12 -0.16  0.53 -0.69 -0.68 -0.37 -0.19 -0.73]
   [ 1.23  1.94 -0.26 -1.29 -0.69 -1.1   0.6  -0.11  1.37]
   [-0.84 -1.   -0.16  0.15  0.12 -0.49 -0.92 -1.04 -0.73]
   [-1.14  0.5  -1.5   0.91  0.77  1.41  5.48 -0.02  1.37]
]
#+end_example






** Data Labeling
#+source: https://aws.amazon.com/sagemaker/groundtruth/what-is-data-labeling/#:~:text=In%20machine%20learning%2C%20data%20labeling,model%20can%20learn%20from%20it.

In Machine Learning, Data labelling is essentially putting a label on a piece of
data to identify that raw data (audio, image, text files, etc). For example when
looking at a photo, and the computer is identifying the object in that photo, it
helps the computer to "learn" by classifying or labelling what that photo
contains... In short, the photo could be labeled "bird" or "card" and the
computer would learn how to identify it.

*** How does Data Labeling work?

So it's essentially using [[[file:../fleeting/202101232318-supervised_learning.org][supervised learning]]] to help train the computer
to identify what object it's learning. This is essentially mapping one input to
one output, we know what the desired output should be, and we need to train the
computer to identify that output.

In Machine Learning, a properly labeled dataset that you use as the objective
standard to train and asses a given model is often called [[[file:../programming/machine_learning/202101252109-ground_truth.org][ground truth]]].

*** What are some commong types of Data Labeling?

Computer Vision: When building a computer vision system, you first need to label
images, pixels, or key points, or create a border that fully encloses a digital
image, known as a bounding box, to generate your training dataset. For example,
you can classify images by quality type (like product vs. lifestyle images) or
content (what’s actually in the image itself), or you can segment an image at
the pixel level. You can then use this training data to build a computer vision
model that can be used to automatically categorize images, detect the location
of objects, identify key points in an image, or segment an image.

Natural Language Processing: Natural language processing requires you to first
manually identify important sections of text or tag the text with specific
labels to generate your training dataset. For example, you may want to identify
the sentiment or intent of a text blurb, identify parts of speech, classify
proper nouns like places and people, and identify text in images, PDFs, or other
files. To do this, you can draw bounding boxes around text and then manually
transcribe the text in your training dataset. Natural language processing models
are used for sentiment analysis, entity name recognition, and optical character
recognition.

Audio Processing: Audio processing converts all kinds of sounds such as speech,
wildlife noises (barks, whistles, or chirps), and building sounds (breaking
glass, scans, or alarms) into a structured format so it can be used in machine
learning. Audio processing often requires you to first manually transcribe it
into written text. From there, you can uncover deeper information about the
audio by adding tags and categorizing the audio. This categorized audio becomes
your training dataset.


*** What are some best practices for Data Labeling?

- Intuitive and streamlined task interfaces :: to help minimize cognitive load
  and context switching for human labelers.
- Labeler consensus :: to help counteract the error/bias of individual
  annotators. Labeler consensus involves sending each dataset object to multiple
  annotators and then consolidating their responses (called “annotations”) into
  a single label.
- Label auditing :: to verify the accuracy of labels and update them as
  necessary.
- Active learning :: to make data labeling more efficient by using machine
  learning to identify the most useful data to be labeled by humans.

*** How can data labeling be done efficiently? :ATTACH:
:PROPERTIES:
:ID:       d4975c6a-25e0-4e1f-9c8a-233e93d43dd2
:END:

Successful machine learning models are built on the shoulders of large volumes
of high-quality training data. But, the process to create the training data
necessary to build these models is often expensive, complicated, and
time-consuming. The majority of models created today require a human to manually
label data in a way that allows the model to learn how to make correct
decisions. To overcome this challenge, labeling can be made more efficient by
using a machine learning model to label data automatically.

In this process, a machine learning model for labeling data is first trained on
a subset of your raw data that has been labeled by humans. Where the labeling
model has high confidence in its results based on what it has learned so far, it
will automatically apply labels to the raw data. Where the labeling model has
lower confidence in its results, it will pass the data to humans to do the
labeling. The human-generated labels are then provided back to the labeling
model for it to learn from and improve its ability to automatically label the
next set of raw data. Over time, the model can label more and more data
automatically and substantially speed up the creation of training datasets.

[[file:../.attach/d4/975c6a-25e0-4e1f-9c8a-233e93d43dd2/_20210125_211455screenshot.png]]

** Data Feature Selection
