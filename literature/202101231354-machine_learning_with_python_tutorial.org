#+title: Machine Learning with Python Tutorial
#+author: Nick Martin
#+email: nmartin84@gmail.com
#+created: [2021-01-23 13:54]
#+source: https://www.tutorialspoint.com/machine_learning_with_python/index.htm

* Machine Learning with Python

** Basics

Machine Learning basically allows the computer to make sense of the data. It
extracts patterns out of data using a set of algorithm and or method.

The idea behind Machine Learning, is to help humans make decisions, based on
data.

*** Machine Learning Model :ATTACH:
:PROPERTIES:
:ID:       741e7e44-ec35-4155-bbd9-80483d28816d
:END:

Before discussing the machine learning model, we must need to understand the
following formal definition of ML given by professor Mitchell:

#+begin_quote
“A computer program is said to learn from experience E with respect to some
class of tasks T and performance measure P, if its performance at tasks in T, as
measured by P, improves with experience E.”
#+end_quote

The above quote basically calls-out the three elements to the machine learning
objective, knowing this we can define Machine Learning as:
+ Improve their performance (P)
+ At executing some task (T)
+ Over time with Experience (E)

[[attachment:machine_learning_model.jpg]]


**** Task (T)

We can define (T "[[[[file:../fleeting/202101231438-task.org][Task]]]]") as the real-world problem to be solved. The problem
which we are trying to solve can be finding housing prices, to marketing
strategy, etc. On the other hand, if we talk about Machine Learning, the
definition for (T) would be different, because our Task that we are trying to
solve is different than the normal.

So basically a (T) Task is said to be based on Machine Learning, when the (T) is
based on the process, and the system must follow for operating on data points.
The examples for Machine Learning are Classification, Regression, Structured
annotation, Clustering, Transcription, etc.

**** Experience (E)

So (E) is essentially our data that's used to build knowledge, which in turn is
applied towards solving our (T) by analyzing and finding patterns.

**** Performance (P)

Our (P) is a measure of how well our Machine Learning algorithm is performing
based on our expectations given its output. (P) is basically a quantitative
metric that tells how a model is performing Task (T), using its Experience (E).
There are many metrics that help understand the Machine Learning performance
(P), such as; accuracy score, F1 score, confusion matrix, precision, recall,
sensitivity, etc...

*** Challenges in Machine Learning

Essentially Machine Learning is still in it's early primes, and has many
improvements to be made, such as in the following areas:
+ quality of data :: Having good-quality data for ML algorithms is one of the
  biggest challenges. Use of low-quality data leads to the problems related to
  data preprocessing and feature extraction.
+ Time-Consuming task :: Another challenge faced by ML models is the consumption
  of time especially for data acquisition, feature extraction and retrieval.
+ Lack of specialist persons :: As ML technology is still in its infancy stage,
  availability of expert resources is a tough job.
+ No clear objective for formulating business problems :: Having no clear
  objective and well-defined goal for business problems is another key
  challenge for ML because this technology is not that mature yet.
+ Issue of overfitting & underfitting :: If the model is overfitting or
  underfitting, it cannot be represented well for the problem.
+ Curse of dimensionality :: Another challenge ML model faces is too many
  features of data points. This can be a real hindrance.
+ Difficulty in deployment :: Complexity of the ML model makes it quite
  difficult to be deployed in real life.

*** Applications of Machine Learning

We are in the midst golden-age of AI and Machine Learning. Some example
applications for machine learning are:
+ Emotion analysis
+ Sentiment analysis
+ Error detection and prevention
+ Weather forecasting and prediction
+ Stock market analysis and forecasting
+ Speech synthesis
+ Speech recognition
+ Customer segmentation
+ Object recognition
+ Fraud detection
+ Fraud prevention
+ Recommendation of products to customer in online shopping

** Methods for Machine Learning
:LOGBOOK:
CLOCK: [2021-01-23 Sat 19:47]--[2021-01-23 Sat 20:12] =>  0:25
:END:

Basically there are multiple Machine Learning algorithms, techniques and methods
that can be applied to build models for solving real-life problems by using data.

*** Different types of Methods

The following are various ML Methods based on some broad categories:

**** Supervised Learning

[[[[file:../fleeting/202101232318-supervised_learning.org][supervised learning]]]]: this is basically taking the input data, and
associating it with the expected outcome results. Some examples of this:
- x: input variables
- y: output variable
- ~Y=f(x)~

The idea would be to approximate the output given the input data we are given.
Supervised algorithms can be categorized as:
+ Classification
+ Regression

***** Classification

Predict the categorical output labels or response for the given input data. The
output will be based on what the model has learned in the training phase.

***** Regression

Predict output labels or responses that are continues numeric values, for the
given input data. The output will be based on what the model has learned in its
training phase.

**** Unsupervised Learning

Opposite of supervised Machine Learning algorithms, which basically means we
have no supervisor to provide any sort of guidance. This is for instance, which
we do not have the liberty of having pre-labeled training data and we want to
extract useful patterns from input data.

For instance, say you have input variables, but there would be no output
variable and the algorithms need to discover the interesting pattern in data for
learning.

Examples of unsupervised learning are: k-means clustering, k-nearest neighbors,
etc...

Unsupervised learning is categorized in the following:

***** Clustering

One of the most powerful unsupervised Machine Learning methods. Used to find
similarity as well as relationship patterns among data samples and then cluster
those samples into groups based on features. An example of this would be
grouping customers by their purchasing behavior.

***** Association

Essentially is another useful algorithm for finding patterns which further
represents the interesting relationships between various items. An example of
this, is to analyze a customers shopping patterns.

***** Dimensionality Reduction

Reduce number of feature variables for each data sample by selecting set of
principal or representative features. Question arises of why we need to reduce
the [[[[file:../fleeting/202101232008-dimensionality.org][dimensionality]]]]? The reason behins is the problem of [[[[file:../fleeting/202101232009-feature_space_complexity.org][feature space
complexity]]]] which arises when we start analyzing and extracting millions of
features from data samples. This problem generally refers to "curse of
[[[[file:../fleeting/202101232008-dimensionality.org][dimensionality]]]]". Principal Component Analysis, K-nearest neighbors and
discriminant analysis are some of the popular algorithms for this purpose.

***** Anomaly Detection

Used to find the occurrences of rare events or observations that generally do
not occur. Some of the unsupervised algorithms like clustering, KNN can detect
anomalies based on the data and its features.
**** Semi-Supervised Learning

Neither supervised nor unsupervised, but a mixture of both. They tend to use a
small amount of pre-labeled annotated data and large unsupervised learning
component, i.e. lots of unlabeled data for training.

+ The first and simple approach is to build with an initial small amount of
  labeled and annotated data, and then build the unsupervised model by applying
  the same to the large amounts of unlabeled data to get more labeled samples.
  Now, train the model on them and repeat the process.
+ Second approach needs some extra efforts. We first use unsupervised methods to
  cluster similar data samples, annotate these groups and then use a combination
  of this information to train the model.

**** Reinforcement Learning

These methods are rarely used. In these type of algorithms, there would be an
agent that we want to train over a period of time so that it can interact with a
specific environment. The agent will follow a set of strategies for interacting
with the environment, and then after observing it will take actions regards the
current state of the environment. Following are the main steps for reinforcement
learning:
- Step 1 :: First, we need to prepare an agent with some initial set of
 strategies.
- Step 2 :: Then observe the environment and its current state.
- Step 3 :: Next, select the optimal policy regards the current state of the
 environment and perform important action.
- Step 4 :: Now, the agent can get corresponding reward or penalty as per
 accordance with the action taken by it in previous step.
- Step 5 :: Now, we can update the strategies if it is required so.
- Step 6 :: At last, repeat steps 2-5 until the agent got to learn and adopt the
 optimal policies.

*** Tasks suited for Machine Learning :ATTACH:
:PROPERTIES:
:ID:       626daac0-c76e-47ff-8a83-a939263a1224
:END:

[[attachment:task_for_ml_problems.jpg]]

**** Batch Learning

In many cases, we have end-to-end Machine Learning systems in which we need to
train the model in one go by using whole available training data. Such kind of
learning method or algorithm is called Batch or Offline learning. It is called
Batch or Offline learning because it is a one-time procedure and the model will
be trained with data in one single batch. The following are the main steps of
Batch learning methods:
- Step 1 :: First, we need to collect all the training data for start training
  the model.
- Step 2 :: Now, start the training of model by providing whole training data in
  one go.
- Step 3 :: Next, stop learning/training process once you got satisfactory
  results/performance.
- Step 4 :: Finally, deploy this trained model into production. Here, it will
  predict the output for new data sample.

**** Online Learning

Is the opposite of batch learning, and the data is provided in multiple
incremental batches, called mini-batches, to the algorithm. Following are the
main steps for online learning:
- Step 1 :: First, we need to collect all the training data for starting
  training of the model.
- Step 2 :: Now, start the training of model by providing a mini-batch of
  training data to the algorithm.
- Step 3 :: Next, we need to provide the mini-batches of training data in
  multiple increments to the algorithm.
- Step 4 :: As it will not stop like batch learning hence after providing whole
  training data in mini-batches, provide new data samples also to it.
- Step 5 :: Finally, it will keep learning over a period of time based on the
  new data samples.

**** Based on Generalization Approach

***** Instance Based Learning
:LOGBOOK:
CLOCK: [2021-01-23 Sat 21:33]
:END:

Instance based is one of the useful Machine Learning methods, by doing
generalization based on the input data. It is opposite of the previously
mentioned learning methods, in that this kind of learning involves Machine
Learning systems as well as methods that uses the raw data points themselves to
draw the outcomes for newer data samples without building an explicit model on
training data.

Or in simpler terms, it'll basically start working by looking at the input data
and then use a similarity metric, it will generalize and predict the new data
points.

***** Model Based Learning

An iterative process takes place on the Machine Learning models that are built
based on various model parameters, called [[[[file:../fleeting/202101232139-hyperparameters.org][hyperparameters]]]] and in which input
data is used to extract features.

In this model, the [[[[file:../fleeting/202101232139-hyperparameters.org][hyperparameters]]]] are used to optimize based on various
model validation techniques.

** Data loading for Machine Learning Projects

The first and foremost thing to start any Machine Learning project, is *data*. The
most common format for any data to be used is *csv*.

*** Considerations while loading CSV data

When loading data from CSV into our Machine Learning project, it is imperative
that we take into consideration the following key items:

**** File Header

The header contains the metadata for the given columns, to help categorize or to
put in summarization what the data for the given column entails. One should keep
in mind when dealing with headers:
+ If a header exists, the header data will be taken into account and labeled
  accordingly.
+ If no header exists, then we need to define our header names to categorize our
  data.

**** Comments

We need to consider that comments may be added to the csv file, comments are
added at the beginning of a line and notated with a ~#~ hashtag.

**** Delimeter

Obviously, the delimeter for which will tell the interpreter how to separate the
data from one another. This is commonly assigned to the ~,~ character.

**** Quotes

It's essential to keep in mind of quotes, because quotes are used to group
together a string of words separated by white-spaces, which when not quoted could
lead to issues.

*** Methods to load CSV data file

So basically this will cover a few functions which can be applied to loading CSV
data into python.

**** Loading CSV with the Python Standard Library

The first and most used approach is using the [[[[file:../programming/python/202101021658-csv.org][csv]]]] module provided with
Python and the ~reader()~ function.

#+begin_example
import csv
import numpy as np

path=r"c:\iris.csv"

with open(path, 'r') as f:
    reader = csv.reader(f, delimter=',')
    headers = next(reader)
    data = list(reader)
    data = np.array(data).astype(float)
#+end_example

**** Load CSV with NumPy

#+begin_example
from numpy import loadtxt
path = r"C:\pima-indians-diabetes.csv"
datapath= open(path, 'r')
data = loadtxt(datapath, delimiter=",")
print(data.shape)
print(data[:3])
#+end_example

**** Load CSV with Pandas

#+begin_example
from pandas import read_csv
path = r"C:\iris.csv"
data = read_csv(path)
print(data.shape)
print(data[:3])
#+end_example

** Understanding Data with Statistics

While working with Machine Learning, two aspect that are often forgotten are the
importance of mathematics and data. Before we can really use Machine Learning to
address our problem, we need to have an understanding of our data first. So
there are two ways to help us understand our data; [[[[file:../statistics/202101212128-statistics.org][statistics]]]] and
*visualization*.

This section is going to cover how we can understand our data through
*statistics*.

*** Looking at Raw Data

The first step is looking at your raw data to get a better understanding of it.
For instance, looking at "India's Diabetes Dataset" and look at the first 50
rows to get a better understanding of our data:

#+begin_example
from pandas import read_csv
path = r"C:\pima-indians-diabetes.csv"
headernames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=headernames)
print(data.head(50))

preg   plas  pres    skin  test  mass   pedi    age      class
0      6      148     72     35   0     33.6    0.627    50    1
1      1       85     66     29   0     26.6    0.351    31    0
2      8      183     64      0   0     23.3    0.672    32    1
3      1       89     66     23  94     28.1    0.167    21    0
4      0      137     40     35  168    43.1    2.288    33    1
5      5      116     74      0   0     25.6    0.201    30    0
6      3       78     50     32   88    31.0    0.248    26    1
7     10      115      0      0   0     35.3    0.134    29    0
8      2      197     70     45  543    30.5    0.158    53    1
9      8      125     96      0   0     0.0     0.232    54    1
10     4      110     92      0   0     37.6    0.191    30    0
11    10      168     74      0   0     38.0    0.537    34    1
12    10      139     80      0   0     27.1    1.441    57    0
13     1      189     60     23  846    30.1    0.398    59    1
14     5      166     72     19  175    25.8    0.587    51    1
15     7      100      0      0   0     30.0    0.484    32    1
16     0      118     84     47  230    45.8    0.551    31    1
17     7      107     74      0   0     29.6    0.254    31    1
18     1      103     30     38  83     43.3    0.183    33    0
19     1      115     70     30  96     34.6    0.529    32    1
20     3      126     88     41  235    39.3    0.704    27    0
21     8       99     84      0   0     35.4    0.388    50    0
22     7      196     90      0   0     39.8    0.451    41    1
23     9      119     80     35   0     29.0    0.263    29    1
24    11      143     94     33  146    36.6    0.254    51    1
25    10      125     70     26  115    31.1    0.205    41    1
26     7      147     76      0   0     39.4    0.257    43    1
27     1       97     66     15  140    23.2    0.487    22    0
28    13      145     82     19  110    22.2    0.245    57    0
29     5      117     92      0   0     34.1    0.337    38    0
30     5      109     75     26   0     36.0    0.546    60    0
31     3      158     76     36  245    31.6    0.851    28    1
32     3       88     58     11   54    24.8    0.267    22    0
33     6       92     92      0   0     19.9    0.188    28    0
34    10      122     78     31   0     27.6    0.512    45    0
35     4      103     60     33  192    24.0    0.966    33    0
36    11      138     76      0   0     33.2    0.420    35    0
37     9      102     76     37   0     32.9    0.665    46    1
38     2       90     68     42   0     38.2    0.503    27    1
39     4      111     72     47  207    37.1    1.390    56    1
40     3      180     64     25   70    34.0    0.271    26    0
41     7      133     84      0   0     40.2    0.696    37    0
42     7      106     92     18   0     22.7    0.235    48    0
43     9      171    110     24  240    45.4    0.721    54    1
44     7      159     64      0   0     27.4    0.294    40    0
45     0      180     66     39   0     42.0    1.893    25    1
46     1      146     56      0   0     29.7    0.564    29    0
47     2       71     70     27   0     28.0    0.586    22    0
48     7      103     66     32   0     39.1    0.344    31    1
49     7      105      0      0   0     0.0     0.305    24    0
#+end_example

So we can observe from the above output, that the first row gives us the index
number which we can use to reference back to a specific observation.

We should also keep in mind the size of our datasets, to see if; The dataset is
very large and could take our algorithm a long term to train, or the dataset is
very small and would provide poor results.

Here with this example from python we can display, or get a rough idea of how
big our dataset is:

#+begin_example
from pandas import read_csv
path = r"C:\iris.csv"
data = read_csv(path)
print(data.shape)

(150, 4)
#+end_example

*** Getting Each Attribute's Data Type

Another key important thing is to know what type of data you are dealing with in
each column, this is particularly important to know so your algorithm is
applying the correct functions according to the respective datatype. Here's an
example of how to print out your datatype for your columns.

#+begin_example
from pandas import read_csv
path = r"C:\iris.csv"
data = read_csv(path)
print(data.dtypes)

sepal_length  float64
sepal_width   float64
petal_length  float64
petal_width   float64
dtype: object
#+end_example

*** Statistical Summary of Data

Many times we would need to review the summaries of our data, which we can do so
with the help of the ~describe()~ function in Pandas that provides the following 8
statistical properties of each & every date attribute:
1. Count
2. Mean
3. Standard Deviation
4. Minimum Value
5. Maximum Value
6. 25%
7. Median i.e. 50%
8. 75%

#+begin_example
from pandas import read_csv
from pandas import set_option
path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=names)
set_option('display.width', 100)
set_option('precision', 2)
print(data.shape)
print(data.describe())

(768, 9)
         preg      plas       pres      skin      test        mass       pedi      age      class
count 768.00      768.00    768.00     768.00    768.00     768.00     768.00    768.00    768.00
mean    3.85      120.89     69.11      20.54     79.80      31.99       0.47     33.24      0.35
std     3.37       31.97     19.36      15.95    115.24       7.88       0.33     11.76      0.48
min     0.00        0.00      0.00       0.00      0.00       0.00       0.08     21.00      0.00
25%     1.00       99.00     62.00       0.00      0.00      27.30       0.24     24.00      0.00
50%     3.00      117.00     72.00      23.00     30.50      32.00       0.37     29.00      0.00
75%     6.00      140.25     80.00      32.00    127.25      36.60       0.63     41.00      1.00
max    17.00      199.00    122.00      99.00    846.00      67.10       2.42     81.00      1.00
#+end_example

*** Reviewing Class Distribution

[[[[file:../fleeting/202101232301-class_distribution.org][class distribution]]]] statistics is useful in classification problems where we
need to know the balance of class values. It's important to know class value
distribution because if we have high imbalanced class distribution, i.e. one
class is having lots more observations than another class, then it ma need
special handling of data preperation stage of our Machine Learning project.

#+begin_example
from pandas import read_csv
path = r"C:\pima-indians-diabetes.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(path, names=names)
count_class = data.groupby('class').size()
print(count_class)

Class
0  500
1  268
dtype: int64
#+end_example

We can see from the example output above, that our class 0 observations are
nearly double that of class 1.

*** Reviewing Correlation between Attributes

The relationship between two variables is called correlation, or better yet
[[[[file:../fleeting/202101232304-variable_correlation.org][variable correlation]]]]. In [[[[file:../statistics/202101212128-statistics.org][statistics]]]], the most efficient method for
calculation correlation is [[[[file:../fleeting/202101232306-pearsons_correlation_coefficient.org][pearsons correlation coefficient]]]]. It can have
three values as follows:
+ Coefficient value = 1 :: Represents full positive correlation between variables.
+ Coefficient value = -1 :: Represents full negative correlation between variables.
+ Coefficient value = 0 :: It represents no correlation at all between variables.

** Understanding data with Visualization
** Preparing data
** Data Feature Selection
